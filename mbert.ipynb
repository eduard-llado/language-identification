{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "068af2c6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-09T17:19:46.232841Z",
     "iopub.status.busy": "2022-04-09T17:19:46.231285Z",
     "iopub.status.idle": "2022-04-09T17:19:55.640371Z",
     "shell.execute_reply": "2022-04-09T17:19:55.639647Z",
     "shell.execute_reply.started": "2022-04-09T14:11:16.542325Z"
    },
    "papermill": {
     "duration": 9.424542,
     "end_time": "2022-04-09T17:19:55.640574",
     "exception": false,
     "start_time": "2022-04-09T17:19:46.216032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 17:19:50.374488: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-09 17:19:50.422023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 17:19:50.504692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 17:19:50.505430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 17:19:52.172230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 17:19:52.173178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 17:19:52.173861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 17:19:52.174494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "['Scripts.txt', 'x_train.txt', 'x_test.txt', 'y_train.txt', 'labels.csv']\n"
     ]
    }
   ],
   "source": [
    "### SETUP\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    pass\n",
    "    #raise SystemError('GPU device not found')\n",
    "    \n",
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch # Deep learning framework\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "import os\n",
    "INPUTDIR = '../input/wili5'\n",
    "print(os.listdir(f'{INPUTDIR}'))\n",
    "\n",
    "#Init random seed to get reproducible results\n",
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.RandomState(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = []\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.token2idx:\n",
    "            self.idx2token.append(token)\n",
    "            self.token2idx[token] = len(self.idx2token) - 1\n",
    "        return self.token2idx[token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2token)\n",
    "    \n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef1b9a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T17:19:55.670097Z",
     "iopub.status.busy": "2022-04-09T17:19:55.669369Z",
     "iopub.status.idle": "2022-04-09T17:19:58.828614Z",
     "shell.execute_reply": "2022-04-09T17:19:58.829083Z",
     "shell.execute_reply.started": "2022-04-09T14:11:33.705225Z"
    },
    "papermill": {
     "duration": 3.176503,
     "end_time": "2022-04-09T17:19:58.829249",
     "exception": false,
     "start_time": "2022-04-09T17:19:55.652746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "LANG = est\n",
      "TEXT = Klement Gottwaldi surnukeha palsameeriti ning paigutati mausoleumi. Surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundemärke. 1962. aastal viidi ta surnukeha mausoleumist ära ja kremeeriti. Zlíni linn kandis aastatel 1949–1989 nime Gottwaldov. Ukrainas Harkivi oblastis kandis Zmiivi linn aastatel 1976–1990 nime Gotvald.\n",
      "Labels: 235 languages\n"
     ]
    }
   ],
   "source": [
    "### LOAD DATA\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "x_train = open(f'{INPUTDIR}/x_train.txt').read().splitlines()\n",
    "y_train = open(f'{INPUTDIR}/y_train.txt').read().splitlines()\n",
    "x_test = open(f'{INPUTDIR}/x_test.txt').read().splitlines()\n",
    "print('Example:')\n",
    "print('LANG =', y_train[0])\n",
    "print('TEXT =', x_train[0])\n",
    "\n",
    "lang_vocab = Dictionary()\n",
    "# use python set to obtain the list of languages without repetitions\n",
    "languages = set(y_train)\n",
    "for lang in sorted(languages):\n",
    "    lang_vocab.add_token(lang)\n",
    "print(\"Labels:\", len(lang_vocab), \"languages\")\n",
    "labels = [lang_vocab.token2idx[lang] for lang in y_train]\n",
    "n_languages = len(lang_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbcfe32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T17:19:58.872366Z",
     "iopub.status.busy": "2022-04-09T17:19:58.871413Z",
     "iopub.status.idle": "2022-04-09T17:31:16.558635Z",
     "shell.execute_reply": "2022-04-09T17:31:16.559081Z",
     "shell.execute_reply.started": "2022-04-09T14:12:33.003074Z"
    },
    "papermill": {
     "duration": 677.718379,
     "end_time": "2022-04-09T17:31:16.559241",
     "exception": false,
     "start_time": "2022-04-09T17:19:58.840862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f508b747a2d4e68b64f546d99c39788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce400047a97444d2996c4369cd9b432e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/851k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac44f63879a42168dfb238483122947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.64M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158f63b9a76741bba9af3b297a6001d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Klement Gottwaldi surnukeha palsameeriti ning paigutati mausoleumi. Surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundemärke. 1962. aastal viidi ta surnukeha mausoleumist ära ja kremeeriti. Zlíni linn kandis aastatel 1949–1989 nime Gottwaldov. Ukrainas Harkivi oblastis kandis Zmiivi linn aastatel 1976–1990 nime Gotvald.\n",
      "Token IDs: tensor([  101, 43298, 14920, 29338, 19629, 10116, 10344, 10999, 10499, 10911,\n",
      "        17993, 26568, 21937, 14119, 11348, 20591, 20472, 13112, 15688, 39891,\n",
      "        26445, 10116,   119, 10344, 10999, 10499, 10911, 10715, 20237, 73061,\n",
      "        42292, 10295, 10209, 10253, 56684, 24383, 10123, 17993, 26568, 21937,\n",
      "        54451, 11348, 76642, 32284, 36377, 17095, 17742, 12650, 30438, 22754,\n",
      "        54662, 10111,   119, 10941,   119, 13048, 13160, 10777, 10546, 10344,\n",
      "        10999, 10499, 10911, 15688, 39891, 26445, 11633, 12805, 10209, 15966,\n",
      "        22023, 72119,   119,   168, 28567, 25481, 36436, 10310, 25683, 11282,\n",
      "          100, 10595, 45444, 29338, 19629, 11123,   119, 30012, 10107, 10347,\n",
      "        10523, 11414, 17729, 10107, 36436, 10310, 15217, 11278, 11414, 25481,\n",
      "        25683, 10792,   100, 10474, 45444, 15517, 58853,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "### ENCODE DATA\n",
    "\n",
    "# INPUT_LENGTH and batch_size can't be too large so model fits in the GPU\n",
    "INPUT_LENGTH = 448   # Maximum of BERT is 512\n",
    "PRETRAINED_MODEL = 'bert-base-multilingual-uncased'\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL, do_lower_case=True)\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in x_train:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = INPUT_LENGTH,           # Pad & truncate all sentences.\n",
    "                        truncation = True,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "\n",
    "for sent in x_test:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = INPUT_LENGTH,           # Pad & truncate all sentences.\n",
    "                        truncation = True,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', x_train[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944cef58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T17:31:16.595569Z",
     "iopub.status.busy": "2022-04-09T17:31:16.595042Z",
     "iopub.status.idle": "2022-04-09T17:31:16.635409Z",
     "shell.execute_reply": "2022-04-09T17:31:16.634981Z",
     "shell.execute_reply.started": "2022-04-09T14:12:51.470221Z"
    },
    "papermill": {
     "duration": 0.062074,
     "end_time": "2022-04-09T17:31:16.635541",
     "exception": false,
     "start_time": "2022-04-09T17:31:16.573467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11,750 validation samples\n"
     ]
    }
   ],
   "source": [
    "### PREPARE BATCH LOADERS\n",
    "\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataset = dataset  # Actually, train with all of the data (so training ~accuracy, not validation)\n",
    "\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "test_ids = torch.tensor(range(len(x_test)))\n",
    "prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_ids)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b08fa60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T17:31:16.675394Z",
     "iopub.status.busy": "2022-04-09T17:31:16.674863Z",
     "iopub.status.idle": "2022-04-09T17:31:43.028989Z",
     "shell.execute_reply": "2022-04-09T17:31:43.029697Z",
     "shell.execute_reply.started": "2022-04-09T14:15:15.842403Z"
    },
    "papermill": {
     "duration": 26.379701,
     "end_time": "2022-04-09T17:31:43.029944",
     "exception": false,
     "start_time": "2022-04-09T17:31:16.650243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ce9e96842448dfb14b184eab035b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/641M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (105879, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                         (235, 768)\n",
      "classifier.bias                                               (235,)\n",
      "\n",
      "==== FULL MODEL ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                         (235, 768)\n",
      "classifier.bias                                               (235,)\n",
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=235, bias=True)\n",
      ")\n",
      "bert.embeddings.word_embeddings.weight 81315072 [105879, 768]\n",
      "bert.embeddings.position_embeddings.weight 393216 [512, 768]\n",
      "bert.embeddings.token_type_embeddings.weight 1536 [2, 768]\n",
      "bert.embeddings.LayerNorm.weight 768 [768]\n",
      "bert.embeddings.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.0.attention.self.query.weight 589824 [768, 768]\n",
      "bert.encoder.layer.0.attention.self.query.bias 768 [768]\n",
      "bert.encoder.layer.0.attention.self.key.weight 589824 [768, 768]\n",
      "bert.encoder.layer.0.attention.self.key.bias 768 [768]\n",
      "bert.encoder.layer.0.attention.self.value.weight 589824 [768, 768]\n",
      "bert.encoder.layer.0.attention.self.value.bias 768 [768]\n",
      "bert.encoder.layer.0.attention.output.dense.weight 589824 [768, 768]\n",
      "bert.encoder.layer.0.attention.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.0.intermediate.dense.weight 2359296 [3072, 768]\n",
      "bert.encoder.layer.0.intermediate.dense.bias 3072 [3072]\n",
      "bert.encoder.layer.0.output.dense.weight 2359296 [768, 3072]\n",
      "bert.encoder.layer.0.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.0.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.0.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.1.attention.self.query.weight 589824 [768, 768]\n",
      "bert.encoder.layer.1.attention.self.query.bias 768 [768]\n",
      "bert.encoder.layer.1.attention.self.key.weight 589824 [768, 768]\n",
      "bert.encoder.layer.1.attention.self.key.bias 768 [768]\n",
      "bert.encoder.layer.1.attention.self.value.weight 589824 [768, 768]\n",
      "bert.encoder.layer.1.attention.self.value.bias 768 [768]\n",
      "bert.encoder.layer.1.attention.output.dense.weight 589824 [768, 768]\n",
      "bert.encoder.layer.1.attention.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.1.intermediate.dense.weight 2359296 [3072, 768]\n",
      "bert.encoder.layer.1.intermediate.dense.bias 3072 [3072]\n",
      "bert.encoder.layer.1.output.dense.weight 2359296 [768, 3072]\n",
      "bert.encoder.layer.1.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.1.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.1.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.2.attention.self.query.weight 589824 [768, 768]\n",
      "bert.encoder.layer.2.attention.self.query.bias 768 [768]\n",
      "bert.encoder.layer.2.attention.self.key.weight 589824 [768, 768]\n",
      "bert.encoder.layer.2.attention.self.key.bias 768 [768]\n",
      "bert.encoder.layer.2.attention.self.value.weight 589824 [768, 768]\n",
      "bert.encoder.layer.2.attention.self.value.bias 768 [768]\n",
      "bert.encoder.layer.2.attention.output.dense.weight 589824 [768, 768]\n",
      "bert.encoder.layer.2.attention.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.2.intermediate.dense.weight 2359296 [3072, 768]\n",
      "bert.encoder.layer.2.intermediate.dense.bias 3072 [3072]\n",
      "bert.encoder.layer.2.output.dense.weight 2359296 [768, 3072]\n",
      "bert.encoder.layer.2.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.2.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.2.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.3.attention.self.query.weight 589824 [768, 768]\n",
      "bert.encoder.layer.3.attention.self.query.bias 768 [768]\n",
      "bert.encoder.layer.3.attention.self.key.weight 589824 [768, 768]\n",
      "bert.encoder.layer.3.attention.self.key.bias 768 [768]\n",
      "bert.encoder.layer.3.attention.self.value.weight 589824 [768, 768]\n",
      "bert.encoder.layer.3.attention.self.value.bias 768 [768]\n",
      "bert.encoder.layer.3.attention.output.dense.weight 589824 [768, 768]\n",
      "bert.encoder.layer.3.attention.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.3.intermediate.dense.weight 2359296 [3072, 768]\n",
      "bert.encoder.layer.3.intermediate.dense.bias 3072 [3072]\n",
      "bert.encoder.layer.3.output.dense.weight 2359296 [768, 3072]\n",
      "bert.encoder.layer.3.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.3.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.3.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.4.attention.self.query.weight 589824 [768, 768]\n",
      "bert.encoder.layer.4.attention.self.query.bias 768 [768]\n",
      "bert.encoder.layer.4.attention.self.key.weight 589824 [768, 768]\n",
      "bert.encoder.layer.4.attention.self.key.bias 768 [768]\n",
      "bert.encoder.layer.4.attention.self.value.weight 589824 [768, 768]\n",
      "bert.encoder.layer.4.attention.self.value.bias 768 [768]\n",
      "bert.encoder.layer.4.attention.output.dense.weight 589824 [768, 768]\n",
      "bert.encoder.layer.4.attention.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.4.intermediate.dense.weight 2359296 [3072, 768]\n",
      "bert.encoder.layer.4.intermediate.dense.bias 3072 [3072]\n",
      "bert.encoder.layer.4.output.dense.weight 2359296 [768, 3072]\n",
      "bert.encoder.layer.4.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.4.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.4.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.5.attention.self.query.weight 589824 [768, 768]\n",
      "bert.encoder.layer.5.attention.self.query.bias 768 [768]\n",
      "bert.encoder.layer.5.attention.self.key.weight 589824 [768, 768]\n",
      "bert.encoder.layer.5.attention.self.key.bias 768 [768]\n",
      "bert.encoder.layer.5.attention.self.value.weight 589824 [768, 768]\n",
      "bert.encoder.layer.5.attention.self.value.bias 768 [768]\n",
      "bert.encoder.layer.5.attention.output.dense.weight 589824 [768, 768]\n",
      "bert.encoder.layer.5.attention.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.5.intermediate.dense.weight 2359296 [3072, 768]\n",
      "bert.encoder.layer.5.intermediate.dense.bias 3072 [3072]\n",
      "bert.encoder.layer.5.output.dense.weight 2359296 [768, 3072]\n",
      "bert.encoder.layer.5.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.5.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.5.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.6.attention.self.query.weight 589824 [768, 768]\n",
      "bert.encoder.layer.6.attention.self.query.bias 768 [768]\n",
      "bert.encoder.layer.6.attention.self.key.weight 589824 [768, 768]\n",
      "bert.encoder.layer.6.attention.self.key.bias 768 [768]\n",
      "bert.encoder.layer.6.attention.self.value.weight 589824 [768, 768]\n",
      "bert.encoder.layer.6.attention.self.value.bias 768 [768]\n",
      "bert.encoder.layer.6.attention.output.dense.weight 589824 [768, 768]\n",
      "bert.encoder.layer.6.attention.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.6.intermediate.dense.weight 2359296 [3072, 768]\n",
      "bert.encoder.layer.6.intermediate.dense.bias 3072 [3072]\n",
      "bert.encoder.layer.6.output.dense.weight 2359296 [768, 3072]\n",
      "bert.encoder.layer.6.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.6.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.6.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.7.attention.self.query.weight 589824 [768, 768]\n",
      "bert.encoder.layer.7.attention.self.query.bias 768 [768]\n",
      "bert.encoder.layer.7.attention.self.key.weight 589824 [768, 768]\n",
      "bert.encoder.layer.7.attention.self.key.bias 768 [768]\n",
      "bert.encoder.layer.7.attention.self.value.weight 589824 [768, 768]\n",
      "bert.encoder.layer.7.attention.self.value.bias 768 [768]\n",
      "bert.encoder.layer.7.attention.output.dense.weight 589824 [768, 768]\n",
      "bert.encoder.layer.7.attention.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.7.intermediate.dense.weight 2359296 [3072, 768]\n",
      "bert.encoder.layer.7.intermediate.dense.bias 3072 [3072]\n",
      "bert.encoder.layer.7.output.dense.weight 2359296 [768, 3072]\n",
      "bert.encoder.layer.7.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.7.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.7.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.8.attention.self.query.weight 589824 [768, 768]\n",
      "bert.encoder.layer.8.attention.self.query.bias 768 [768]\n",
      "bert.encoder.layer.8.attention.self.key.weight 589824 [768, 768]\n",
      "bert.encoder.layer.8.attention.self.key.bias 768 [768]\n",
      "bert.encoder.layer.8.attention.self.value.weight 589824 [768, 768]\n",
      "bert.encoder.layer.8.attention.self.value.bias 768 [768]\n",
      "bert.encoder.layer.8.attention.output.dense.weight 589824 [768, 768]\n",
      "bert.encoder.layer.8.attention.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.8.intermediate.dense.weight 2359296 [3072, 768]\n",
      "bert.encoder.layer.8.intermediate.dense.bias 3072 [3072]\n",
      "bert.encoder.layer.8.output.dense.weight 2359296 [768, 3072]\n",
      "bert.encoder.layer.8.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.8.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.8.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.9.attention.self.query.weight 589824 [768, 768]\n",
      "bert.encoder.layer.9.attention.self.query.bias 768 [768]\n",
      "bert.encoder.layer.9.attention.self.key.weight 589824 [768, 768]\n",
      "bert.encoder.layer.9.attention.self.key.bias 768 [768]\n",
      "bert.encoder.layer.9.attention.self.value.weight 589824 [768, 768]\n",
      "bert.encoder.layer.9.attention.self.value.bias 768 [768]\n",
      "bert.encoder.layer.9.attention.output.dense.weight 589824 [768, 768]\n",
      "bert.encoder.layer.9.attention.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.9.intermediate.dense.weight 2359296 [3072, 768]\n",
      "bert.encoder.layer.9.intermediate.dense.bias 3072 [3072]\n",
      "bert.encoder.layer.9.output.dense.weight 2359296 [768, 3072]\n",
      "bert.encoder.layer.9.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.9.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.9.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.10.attention.self.query.weight 589824 [768, 768]\n",
      "bert.encoder.layer.10.attention.self.query.bias 768 [768]\n",
      "bert.encoder.layer.10.attention.self.key.weight 589824 [768, 768]\n",
      "bert.encoder.layer.10.attention.self.key.bias 768 [768]\n",
      "bert.encoder.layer.10.attention.self.value.weight 589824 [768, 768]\n",
      "bert.encoder.layer.10.attention.self.value.bias 768 [768]\n",
      "bert.encoder.layer.10.attention.output.dense.weight 589824 [768, 768]\n",
      "bert.encoder.layer.10.attention.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.10.intermediate.dense.weight 2359296 [3072, 768]\n",
      "bert.encoder.layer.10.intermediate.dense.bias 3072 [3072]\n",
      "bert.encoder.layer.10.output.dense.weight 2359296 [768, 3072]\n",
      "bert.encoder.layer.10.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.10.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.10.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.11.attention.self.query.weight 589824 [768, 768]\n",
      "bert.encoder.layer.11.attention.self.query.bias 768 [768]\n",
      "bert.encoder.layer.11.attention.self.key.weight 589824 [768, 768]\n",
      "bert.encoder.layer.11.attention.self.key.bias 768 [768]\n",
      "bert.encoder.layer.11.attention.self.value.weight 589824 [768, 768]\n",
      "bert.encoder.layer.11.attention.self.value.bias 768 [768]\n",
      "bert.encoder.layer.11.attention.output.dense.weight 589824 [768, 768]\n",
      "bert.encoder.layer.11.attention.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias 768 [768]\n",
      "bert.encoder.layer.11.intermediate.dense.weight 2359296 [3072, 768]\n",
      "bert.encoder.layer.11.intermediate.dense.bias 3072 [3072]\n",
      "bert.encoder.layer.11.output.dense.weight 2359296 [768, 3072]\n",
      "bert.encoder.layer.11.output.dense.bias 768 [768]\n",
      "bert.encoder.layer.11.output.LayerNorm.weight 768 [768]\n",
      "bert.encoder.layer.11.output.LayerNorm.bias 768 [768]\n",
      "bert.pooler.dense.weight 589824 [768, 768]\n",
      "bert.pooler.dense.bias 768 [768]\n",
      "classifier.weight    180480 [235, 768]\n",
      "classifier.bias      235 [235]\n",
      "TOTAL                167537131\n"
     ]
    }
   ],
   "source": [
    "### DEFINE MODEL\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    num_labels = n_languages, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()\n",
    "\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "    \n",
    "print('\\n==== FULL MODEL ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'{name:20} {param.numel()} {list(param.shape)}')\n",
    "print(f'TOTAL                {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b6f8bbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T17:31:43.096922Z",
     "iopub.status.busy": "2022-04-09T17:31:43.096029Z",
     "iopub.status.idle": "2022-04-09T23:42:20.680246Z",
     "shell.execute_reply": "2022-04-09T23:42:20.680670Z",
     "shell.execute_reply.started": "2022-04-09T14:16:08.055892Z"
    },
    "papermill": {
     "duration": 22237.622383,
     "end_time": "2022-04-09T23:42:20.680818",
     "exception": false,
     "start_time": "2022-04-09T17:31:43.058435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  7,344.    Elapsed: 0:00:30.\n",
      "  Batch    80  of  7,344.    Elapsed: 0:01:00.\n",
      "  Batch   120  of  7,344.    Elapsed: 0:01:29.\n",
      "  Batch   160  of  7,344.    Elapsed: 0:01:58.\n",
      "  Batch   200  of  7,344.    Elapsed: 0:02:28.\n",
      "  Batch   240  of  7,344.    Elapsed: 0:02:57.\n",
      "  Batch   280  of  7,344.    Elapsed: 0:03:26.\n",
      "  Batch   320  of  7,344.    Elapsed: 0:03:56.\n",
      "  Batch   360  of  7,344.    Elapsed: 0:04:25.\n",
      "  Batch   400  of  7,344.    Elapsed: 0:04:55.\n",
      "  Batch   440  of  7,344.    Elapsed: 0:05:24.\n",
      "  Batch   480  of  7,344.    Elapsed: 0:05:54.\n",
      "  Batch   520  of  7,344.    Elapsed: 0:06:23.\n",
      "  Batch   560  of  7,344.    Elapsed: 0:06:52.\n",
      "  Batch   600  of  7,344.    Elapsed: 0:07:22.\n",
      "  Batch   640  of  7,344.    Elapsed: 0:07:51.\n",
      "  Batch   680  of  7,344.    Elapsed: 0:08:20.\n",
      "  Batch   720  of  7,344.    Elapsed: 0:08:50.\n",
      "  Batch   760  of  7,344.    Elapsed: 0:09:19.\n",
      "  Batch   800  of  7,344.    Elapsed: 0:09:49.\n",
      "  Batch   840  of  7,344.    Elapsed: 0:10:18.\n",
      "  Batch   880  of  7,344.    Elapsed: 0:10:47.\n",
      "  Batch   920  of  7,344.    Elapsed: 0:11:17.\n",
      "  Batch   960  of  7,344.    Elapsed: 0:11:46.\n",
      "  Batch 1,000  of  7,344.    Elapsed: 0:12:15.\n",
      "  Batch 1,040  of  7,344.    Elapsed: 0:12:45.\n",
      "  Batch 1,080  of  7,344.    Elapsed: 0:13:14.\n",
      "  Batch 1,120  of  7,344.    Elapsed: 0:13:44.\n",
      "  Batch 1,160  of  7,344.    Elapsed: 0:14:13.\n",
      "  Batch 1,200  of  7,344.    Elapsed: 0:14:42.\n",
      "  Batch 1,240  of  7,344.    Elapsed: 0:15:12.\n",
      "  Batch 1,280  of  7,344.    Elapsed: 0:15:41.\n",
      "  Batch 1,320  of  7,344.    Elapsed: 0:16:10.\n",
      "  Batch 1,360  of  7,344.    Elapsed: 0:16:40.\n",
      "  Batch 1,400  of  7,344.    Elapsed: 0:17:09.\n",
      "  Batch 1,440  of  7,344.    Elapsed: 0:17:39.\n",
      "  Batch 1,480  of  7,344.    Elapsed: 0:18:08.\n",
      "  Batch 1,520  of  7,344.    Elapsed: 0:18:37.\n",
      "  Batch 1,560  of  7,344.    Elapsed: 0:19:07.\n",
      "  Batch 1,600  of  7,344.    Elapsed: 0:19:36.\n",
      "  Batch 1,640  of  7,344.    Elapsed: 0:20:05.\n",
      "  Batch 1,680  of  7,344.    Elapsed: 0:20:35.\n",
      "  Batch 1,720  of  7,344.    Elapsed: 0:21:04.\n",
      "  Batch 1,760  of  7,344.    Elapsed: 0:21:33.\n",
      "  Batch 1,800  of  7,344.    Elapsed: 0:22:03.\n",
      "  Batch 1,840  of  7,344.    Elapsed: 0:22:32.\n",
      "  Batch 1,880  of  7,344.    Elapsed: 0:23:02.\n",
      "  Batch 1,920  of  7,344.    Elapsed: 0:23:31.\n",
      "  Batch 1,960  of  7,344.    Elapsed: 0:24:00.\n",
      "  Batch 2,000  of  7,344.    Elapsed: 0:24:30.\n",
      "  Batch 2,040  of  7,344.    Elapsed: 0:24:59.\n",
      "  Batch 2,080  of  7,344.    Elapsed: 0:25:28.\n",
      "  Batch 2,120  of  7,344.    Elapsed: 0:25:58.\n",
      "  Batch 2,160  of  7,344.    Elapsed: 0:26:27.\n",
      "  Batch 2,200  of  7,344.    Elapsed: 0:26:56.\n",
      "  Batch 2,240  of  7,344.    Elapsed: 0:27:26.\n",
      "  Batch 2,280  of  7,344.    Elapsed: 0:27:55.\n",
      "  Batch 2,320  of  7,344.    Elapsed: 0:28:25.\n",
      "  Batch 2,360  of  7,344.    Elapsed: 0:28:54.\n",
      "  Batch 2,400  of  7,344.    Elapsed: 0:29:23.\n",
      "  Batch 2,440  of  7,344.    Elapsed: 0:29:53.\n",
      "  Batch 2,480  of  7,344.    Elapsed: 0:30:22.\n",
      "  Batch 2,520  of  7,344.    Elapsed: 0:30:52.\n",
      "  Batch 2,560  of  7,344.    Elapsed: 0:31:21.\n",
      "  Batch 2,600  of  7,344.    Elapsed: 0:31:50.\n",
      "  Batch 2,640  of  7,344.    Elapsed: 0:32:20.\n",
      "  Batch 2,680  of  7,344.    Elapsed: 0:32:49.\n",
      "  Batch 2,720  of  7,344.    Elapsed: 0:33:18.\n",
      "  Batch 2,760  of  7,344.    Elapsed: 0:33:48.\n",
      "  Batch 2,800  of  7,344.    Elapsed: 0:34:17.\n",
      "  Batch 2,840  of  7,344.    Elapsed: 0:34:47.\n",
      "  Batch 2,880  of  7,344.    Elapsed: 0:35:16.\n",
      "  Batch 2,920  of  7,344.    Elapsed: 0:35:45.\n",
      "  Batch 2,960  of  7,344.    Elapsed: 0:36:15.\n",
      "  Batch 3,000  of  7,344.    Elapsed: 0:36:44.\n",
      "  Batch 3,040  of  7,344.    Elapsed: 0:37:14.\n",
      "  Batch 3,080  of  7,344.    Elapsed: 0:37:43.\n",
      "  Batch 3,120  of  7,344.    Elapsed: 0:38:12.\n",
      "  Batch 3,160  of  7,344.    Elapsed: 0:38:42.\n",
      "  Batch 3,200  of  7,344.    Elapsed: 0:39:11.\n",
      "  Batch 3,240  of  7,344.    Elapsed: 0:39:40.\n",
      "  Batch 3,280  of  7,344.    Elapsed: 0:40:10.\n",
      "  Batch 3,320  of  7,344.    Elapsed: 0:40:39.\n",
      "  Batch 3,360  of  7,344.    Elapsed: 0:41:09.\n",
      "  Batch 3,400  of  7,344.    Elapsed: 0:41:38.\n",
      "  Batch 3,440  of  7,344.    Elapsed: 0:42:07.\n",
      "  Batch 3,480  of  7,344.    Elapsed: 0:42:37.\n",
      "  Batch 3,520  of  7,344.    Elapsed: 0:43:06.\n",
      "  Batch 3,560  of  7,344.    Elapsed: 0:43:35.\n",
      "  Batch 3,600  of  7,344.    Elapsed: 0:44:05.\n",
      "  Batch 3,640  of  7,344.    Elapsed: 0:44:34.\n",
      "  Batch 3,680  of  7,344.    Elapsed: 0:45:03.\n",
      "  Batch 3,720  of  7,344.    Elapsed: 0:45:33.\n",
      "  Batch 3,760  of  7,344.    Elapsed: 0:46:02.\n",
      "  Batch 3,800  of  7,344.    Elapsed: 0:46:32.\n",
      "  Batch 3,840  of  7,344.    Elapsed: 0:47:01.\n",
      "  Batch 3,880  of  7,344.    Elapsed: 0:47:30.\n",
      "  Batch 3,920  of  7,344.    Elapsed: 0:48:00.\n",
      "  Batch 3,960  of  7,344.    Elapsed: 0:48:29.\n",
      "  Batch 4,000  of  7,344.    Elapsed: 0:48:59.\n",
      "  Batch 4,040  of  7,344.    Elapsed: 0:49:28.\n",
      "  Batch 4,080  of  7,344.    Elapsed: 0:49:57.\n",
      "  Batch 4,120  of  7,344.    Elapsed: 0:50:27.\n",
      "  Batch 4,160  of  7,344.    Elapsed: 0:50:56.\n",
      "  Batch 4,200  of  7,344.    Elapsed: 0:51:25.\n",
      "  Batch 4,240  of  7,344.    Elapsed: 0:51:55.\n",
      "  Batch 4,280  of  7,344.    Elapsed: 0:52:24.\n",
      "  Batch 4,320  of  7,344.    Elapsed: 0:52:54.\n",
      "  Batch 4,360  of  7,344.    Elapsed: 0:53:23.\n",
      "  Batch 4,400  of  7,344.    Elapsed: 0:53:52.\n",
      "  Batch 4,440  of  7,344.    Elapsed: 0:54:22.\n",
      "  Batch 4,480  of  7,344.    Elapsed: 0:54:51.\n",
      "  Batch 4,520  of  7,344.    Elapsed: 0:55:21.\n",
      "  Batch 4,560  of  7,344.    Elapsed: 0:55:50.\n",
      "  Batch 4,600  of  7,344.    Elapsed: 0:56:19.\n",
      "  Batch 4,640  of  7,344.    Elapsed: 0:56:49.\n",
      "  Batch 4,680  of  7,344.    Elapsed: 0:57:18.\n",
      "  Batch 4,720  of  7,344.    Elapsed: 0:57:48.\n",
      "  Batch 4,760  of  7,344.    Elapsed: 0:58:17.\n",
      "  Batch 4,800  of  7,344.    Elapsed: 0:58:46.\n",
      "  Batch 4,840  of  7,344.    Elapsed: 0:59:16.\n",
      "  Batch 4,880  of  7,344.    Elapsed: 0:59:45.\n",
      "  Batch 4,920  of  7,344.    Elapsed: 1:00:15.\n",
      "  Batch 4,960  of  7,344.    Elapsed: 1:00:44.\n",
      "  Batch 5,000  of  7,344.    Elapsed: 1:01:13.\n",
      "  Batch 5,040  of  7,344.    Elapsed: 1:01:43.\n",
      "  Batch 5,080  of  7,344.    Elapsed: 1:02:12.\n",
      "  Batch 5,120  of  7,344.    Elapsed: 1:02:42.\n",
      "  Batch 5,160  of  7,344.    Elapsed: 1:03:11.\n",
      "  Batch 5,200  of  7,344.    Elapsed: 1:03:40.\n",
      "  Batch 5,240  of  7,344.    Elapsed: 1:04:10.\n",
      "  Batch 5,280  of  7,344.    Elapsed: 1:04:39.\n",
      "  Batch 5,320  of  7,344.    Elapsed: 1:05:08.\n",
      "  Batch 5,360  of  7,344.    Elapsed: 1:05:38.\n",
      "  Batch 5,400  of  7,344.    Elapsed: 1:06:07.\n",
      "  Batch 5,440  of  7,344.    Elapsed: 1:06:37.\n",
      "  Batch 5,480  of  7,344.    Elapsed: 1:07:06.\n",
      "  Batch 5,520  of  7,344.    Elapsed: 1:07:35.\n",
      "  Batch 5,560  of  7,344.    Elapsed: 1:08:05.\n",
      "  Batch 5,600  of  7,344.    Elapsed: 1:08:34.\n",
      "  Batch 5,640  of  7,344.    Elapsed: 1:09:03.\n",
      "  Batch 5,680  of  7,344.    Elapsed: 1:09:33.\n",
      "  Batch 5,720  of  7,344.    Elapsed: 1:10:02.\n",
      "  Batch 5,760  of  7,344.    Elapsed: 1:10:32.\n",
      "  Batch 5,800  of  7,344.    Elapsed: 1:11:01.\n",
      "  Batch 5,840  of  7,344.    Elapsed: 1:11:30.\n",
      "  Batch 5,880  of  7,344.    Elapsed: 1:12:00.\n",
      "  Batch 5,920  of  7,344.    Elapsed: 1:12:29.\n",
      "  Batch 5,960  of  7,344.    Elapsed: 1:12:58.\n",
      "  Batch 6,000  of  7,344.    Elapsed: 1:13:28.\n",
      "  Batch 6,040  of  7,344.    Elapsed: 1:13:57.\n",
      "  Batch 6,080  of  7,344.    Elapsed: 1:14:27.\n",
      "  Batch 6,120  of  7,344.    Elapsed: 1:14:56.\n",
      "  Batch 6,160  of  7,344.    Elapsed: 1:15:26.\n",
      "  Batch 6,200  of  7,344.    Elapsed: 1:15:55.\n",
      "  Batch 6,240  of  7,344.    Elapsed: 1:16:24.\n",
      "  Batch 6,280  of  7,344.    Elapsed: 1:16:54.\n",
      "  Batch 6,320  of  7,344.    Elapsed: 1:17:23.\n",
      "  Batch 6,360  of  7,344.    Elapsed: 1:17:52.\n",
      "  Batch 6,400  of  7,344.    Elapsed: 1:18:22.\n",
      "  Batch 6,440  of  7,344.    Elapsed: 1:18:51.\n",
      "  Batch 6,480  of  7,344.    Elapsed: 1:19:21.\n",
      "  Batch 6,520  of  7,344.    Elapsed: 1:19:50.\n",
      "  Batch 6,560  of  7,344.    Elapsed: 1:20:19.\n",
      "  Batch 6,600  of  7,344.    Elapsed: 1:20:49.\n",
      "  Batch 6,640  of  7,344.    Elapsed: 1:21:18.\n",
      "  Batch 6,680  of  7,344.    Elapsed: 1:21:48.\n",
      "  Batch 6,720  of  7,344.    Elapsed: 1:22:17.\n",
      "  Batch 6,760  of  7,344.    Elapsed: 1:22:46.\n",
      "  Batch 6,800  of  7,344.    Elapsed: 1:23:16.\n",
      "  Batch 6,840  of  7,344.    Elapsed: 1:23:45.\n",
      "  Batch 6,880  of  7,344.    Elapsed: 1:24:15.\n",
      "  Batch 6,920  of  7,344.    Elapsed: 1:24:44.\n",
      "  Batch 6,960  of  7,344.    Elapsed: 1:25:13.\n",
      "  Batch 7,000  of  7,344.    Elapsed: 1:25:43.\n",
      "  Batch 7,040  of  7,344.    Elapsed: 1:26:12.\n",
      "  Batch 7,080  of  7,344.    Elapsed: 1:26:41.\n",
      "  Batch 7,120  of  7,344.    Elapsed: 1:27:11.\n",
      "  Batch 7,160  of  7,344.    Elapsed: 1:27:40.\n",
      "  Batch 7,200  of  7,344.    Elapsed: 1:28:10.\n",
      "  Batch 7,240  of  7,344.    Elapsed: 1:28:39.\n",
      "  Batch 7,280  of  7,344.    Elapsed: 1:29:08.\n",
      "  Batch 7,320  of  7,344.    Elapsed: 1:29:38.\n",
      "\n",
      "  Average training loss: 0.90\n",
      "  Training epcoh took: 1:29:55\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.16\n",
      "  Validation took: 0:02:52\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  7,344.    Elapsed: 0:00:29.\n",
      "  Batch    80  of  7,344.    Elapsed: 0:00:59.\n",
      "  Batch   120  of  7,344.    Elapsed: 0:01:28.\n",
      "  Batch   160  of  7,344.    Elapsed: 0:01:58.\n",
      "  Batch   200  of  7,344.    Elapsed: 0:02:27.\n",
      "  Batch   240  of  7,344.    Elapsed: 0:02:56.\n",
      "  Batch   280  of  7,344.    Elapsed: 0:03:26.\n",
      "  Batch   320  of  7,344.    Elapsed: 0:03:55.\n",
      "  Batch   360  of  7,344.    Elapsed: 0:04:24.\n",
      "  Batch   400  of  7,344.    Elapsed: 0:04:54.\n",
      "  Batch   440  of  7,344.    Elapsed: 0:05:23.\n",
      "  Batch   480  of  7,344.    Elapsed: 0:05:52.\n",
      "  Batch   520  of  7,344.    Elapsed: 0:06:22.\n",
      "  Batch   560  of  7,344.    Elapsed: 0:06:51.\n",
      "  Batch   600  of  7,344.    Elapsed: 0:07:21.\n",
      "  Batch   640  of  7,344.    Elapsed: 0:07:50.\n",
      "  Batch   680  of  7,344.    Elapsed: 0:08:19.\n",
      "  Batch   720  of  7,344.    Elapsed: 0:08:49.\n",
      "  Batch   760  of  7,344.    Elapsed: 0:09:18.\n",
      "  Batch   800  of  7,344.    Elapsed: 0:09:47.\n",
      "  Batch   840  of  7,344.    Elapsed: 0:10:17.\n",
      "  Batch   880  of  7,344.    Elapsed: 0:10:46.\n",
      "  Batch   920  of  7,344.    Elapsed: 0:11:16.\n",
      "  Batch   960  of  7,344.    Elapsed: 0:11:45.\n",
      "  Batch 1,000  of  7,344.    Elapsed: 0:12:14.\n",
      "  Batch 1,040  of  7,344.    Elapsed: 0:12:44.\n",
      "  Batch 1,080  of  7,344.    Elapsed: 0:13:13.\n",
      "  Batch 1,120  of  7,344.    Elapsed: 0:13:42.\n",
      "  Batch 1,160  of  7,344.    Elapsed: 0:14:12.\n",
      "  Batch 1,200  of  7,344.    Elapsed: 0:14:41.\n",
      "  Batch 1,240  of  7,344.    Elapsed: 0:15:10.\n",
      "  Batch 1,280  of  7,344.    Elapsed: 0:15:40.\n",
      "  Batch 1,320  of  7,344.    Elapsed: 0:16:09.\n",
      "  Batch 1,360  of  7,344.    Elapsed: 0:16:39.\n",
      "  Batch 1,400  of  7,344.    Elapsed: 0:17:08.\n",
      "  Batch 1,440  of  7,344.    Elapsed: 0:17:37.\n",
      "  Batch 1,480  of  7,344.    Elapsed: 0:18:07.\n",
      "  Batch 1,520  of  7,344.    Elapsed: 0:18:36.\n",
      "  Batch 1,560  of  7,344.    Elapsed: 0:19:06.\n",
      "  Batch 1,600  of  7,344.    Elapsed: 0:19:35.\n",
      "  Batch 1,640  of  7,344.    Elapsed: 0:20:04.\n",
      "  Batch 1,680  of  7,344.    Elapsed: 0:20:34.\n",
      "  Batch 1,720  of  7,344.    Elapsed: 0:21:03.\n",
      "  Batch 1,760  of  7,344.    Elapsed: 0:21:32.\n",
      "  Batch 1,800  of  7,344.    Elapsed: 0:22:02.\n",
      "  Batch 1,840  of  7,344.    Elapsed: 0:22:31.\n",
      "  Batch 1,880  of  7,344.    Elapsed: 0:23:00.\n",
      "  Batch 1,920  of  7,344.    Elapsed: 0:23:30.\n",
      "  Batch 1,960  of  7,344.    Elapsed: 0:23:59.\n",
      "  Batch 2,000  of  7,344.    Elapsed: 0:24:29.\n",
      "  Batch 2,040  of  7,344.    Elapsed: 0:24:58.\n",
      "  Batch 2,080  of  7,344.    Elapsed: 0:25:27.\n",
      "  Batch 2,120  of  7,344.    Elapsed: 0:25:57.\n",
      "  Batch 2,160  of  7,344.    Elapsed: 0:26:26.\n",
      "  Batch 2,200  of  7,344.    Elapsed: 0:26:55.\n",
      "  Batch 2,240  of  7,344.    Elapsed: 0:27:25.\n",
      "  Batch 2,280  of  7,344.    Elapsed: 0:27:54.\n",
      "  Batch 2,320  of  7,344.    Elapsed: 0:28:24.\n",
      "  Batch 2,360  of  7,344.    Elapsed: 0:28:53.\n",
      "  Batch 2,400  of  7,344.    Elapsed: 0:29:22.\n",
      "  Batch 2,440  of  7,344.    Elapsed: 0:29:52.\n",
      "  Batch 2,480  of  7,344.    Elapsed: 0:30:21.\n",
      "  Batch 2,520  of  7,344.    Elapsed: 0:30:50.\n",
      "  Batch 2,560  of  7,344.    Elapsed: 0:31:20.\n",
      "  Batch 2,600  of  7,344.    Elapsed: 0:31:49.\n",
      "  Batch 2,640  of  7,344.    Elapsed: 0:32:19.\n",
      "  Batch 2,680  of  7,344.    Elapsed: 0:32:48.\n",
      "  Batch 2,720  of  7,344.    Elapsed: 0:33:17.\n",
      "  Batch 2,760  of  7,344.    Elapsed: 0:33:47.\n",
      "  Batch 2,800  of  7,344.    Elapsed: 0:34:16.\n",
      "  Batch 2,840  of  7,344.    Elapsed: 0:34:45.\n",
      "  Batch 2,880  of  7,344.    Elapsed: 0:35:15.\n",
      "  Batch 2,920  of  7,344.    Elapsed: 0:35:44.\n",
      "  Batch 2,960  of  7,344.    Elapsed: 0:36:13.\n",
      "  Batch 3,000  of  7,344.    Elapsed: 0:36:43.\n",
      "  Batch 3,040  of  7,344.    Elapsed: 0:37:12.\n",
      "  Batch 3,080  of  7,344.    Elapsed: 0:37:42.\n",
      "  Batch 3,120  of  7,344.    Elapsed: 0:38:11.\n",
      "  Batch 3,160  of  7,344.    Elapsed: 0:38:40.\n",
      "  Batch 3,200  of  7,344.    Elapsed: 0:39:10.\n",
      "  Batch 3,240  of  7,344.    Elapsed: 0:39:39.\n",
      "  Batch 3,280  of  7,344.    Elapsed: 0:40:09.\n",
      "  Batch 3,320  of  7,344.    Elapsed: 0:40:38.\n",
      "  Batch 3,360  of  7,344.    Elapsed: 0:41:07.\n",
      "  Batch 3,400  of  7,344.    Elapsed: 0:41:37.\n",
      "  Batch 3,440  of  7,344.    Elapsed: 0:42:06.\n",
      "  Batch 3,480  of  7,344.    Elapsed: 0:42:36.\n",
      "  Batch 3,520  of  7,344.    Elapsed: 0:43:05.\n",
      "  Batch 3,560  of  7,344.    Elapsed: 0:43:34.\n",
      "  Batch 3,600  of  7,344.    Elapsed: 0:44:04.\n",
      "  Batch 3,640  of  7,344.    Elapsed: 0:44:33.\n",
      "  Batch 3,680  of  7,344.    Elapsed: 0:45:03.\n",
      "  Batch 3,720  of  7,344.    Elapsed: 0:45:32.\n",
      "  Batch 3,760  of  7,344.    Elapsed: 0:46:01.\n",
      "  Batch 3,800  of  7,344.    Elapsed: 0:46:31.\n",
      "  Batch 3,840  of  7,344.    Elapsed: 0:47:00.\n",
      "  Batch 3,880  of  7,344.    Elapsed: 0:47:29.\n",
      "  Batch 3,920  of  7,344.    Elapsed: 0:47:59.\n",
      "  Batch 3,960  of  7,344.    Elapsed: 0:48:28.\n",
      "  Batch 4,000  of  7,344.    Elapsed: 0:48:57.\n",
      "  Batch 4,040  of  7,344.    Elapsed: 0:49:27.\n",
      "  Batch 4,080  of  7,344.    Elapsed: 0:49:56.\n",
      "  Batch 4,120  of  7,344.    Elapsed: 0:50:25.\n",
      "  Batch 4,160  of  7,344.    Elapsed: 0:50:55.\n",
      "  Batch 4,200  of  7,344.    Elapsed: 0:51:24.\n",
      "  Batch 4,240  of  7,344.    Elapsed: 0:51:53.\n",
      "  Batch 4,280  of  7,344.    Elapsed: 0:52:23.\n",
      "  Batch 4,320  of  7,344.    Elapsed: 0:52:52.\n",
      "  Batch 4,360  of  7,344.    Elapsed: 0:53:22.\n",
      "  Batch 4,400  of  7,344.    Elapsed: 0:53:51.\n",
      "  Batch 4,440  of  7,344.    Elapsed: 0:54:20.\n",
      "  Batch 4,480  of  7,344.    Elapsed: 0:54:50.\n",
      "  Batch 4,520  of  7,344.    Elapsed: 0:55:19.\n",
      "  Batch 4,560  of  7,344.    Elapsed: 0:55:48.\n",
      "  Batch 4,600  of  7,344.    Elapsed: 0:56:18.\n",
      "  Batch 4,640  of  7,344.    Elapsed: 0:56:47.\n",
      "  Batch 4,680  of  7,344.    Elapsed: 0:57:17.\n",
      "  Batch 4,720  of  7,344.    Elapsed: 0:57:46.\n",
      "  Batch 4,760  of  7,344.    Elapsed: 0:58:15.\n",
      "  Batch 4,800  of  7,344.    Elapsed: 0:58:45.\n",
      "  Batch 4,840  of  7,344.    Elapsed: 0:59:14.\n",
      "  Batch 4,880  of  7,344.    Elapsed: 0:59:43.\n",
      "  Batch 4,920  of  7,344.    Elapsed: 1:00:13.\n",
      "  Batch 4,960  of  7,344.    Elapsed: 1:00:42.\n",
      "  Batch 5,000  of  7,344.    Elapsed: 1:01:11.\n",
      "  Batch 5,040  of  7,344.    Elapsed: 1:01:41.\n",
      "  Batch 5,080  of  7,344.    Elapsed: 1:02:10.\n",
      "  Batch 5,120  of  7,344.    Elapsed: 1:02:40.\n",
      "  Batch 5,160  of  7,344.    Elapsed: 1:03:09.\n",
      "  Batch 5,200  of  7,344.    Elapsed: 1:03:38.\n",
      "  Batch 5,240  of  7,344.    Elapsed: 1:04:08.\n",
      "  Batch 5,280  of  7,344.    Elapsed: 1:04:37.\n",
      "  Batch 5,320  of  7,344.    Elapsed: 1:05:06.\n",
      "  Batch 5,360  of  7,344.    Elapsed: 1:05:36.\n",
      "  Batch 5,400  of  7,344.    Elapsed: 1:06:05.\n",
      "  Batch 5,440  of  7,344.    Elapsed: 1:06:34.\n",
      "  Batch 5,480  of  7,344.    Elapsed: 1:07:04.\n",
      "  Batch 5,520  of  7,344.    Elapsed: 1:07:33.\n",
      "  Batch 5,560  of  7,344.    Elapsed: 1:08:03.\n",
      "  Batch 5,600  of  7,344.    Elapsed: 1:08:32.\n",
      "  Batch 5,640  of  7,344.    Elapsed: 1:09:01.\n",
      "  Batch 5,680  of  7,344.    Elapsed: 1:09:31.\n",
      "  Batch 5,720  of  7,344.    Elapsed: 1:10:00.\n",
      "  Batch 5,760  of  7,344.    Elapsed: 1:10:29.\n",
      "  Batch 5,800  of  7,344.    Elapsed: 1:10:59.\n",
      "  Batch 5,840  of  7,344.    Elapsed: 1:11:28.\n",
      "  Batch 5,880  of  7,344.    Elapsed: 1:11:57.\n",
      "  Batch 5,920  of  7,344.    Elapsed: 1:12:27.\n",
      "  Batch 5,960  of  7,344.    Elapsed: 1:12:56.\n",
      "  Batch 6,000  of  7,344.    Elapsed: 1:13:25.\n",
      "  Batch 6,040  of  7,344.    Elapsed: 1:13:55.\n",
      "  Batch 6,080  of  7,344.    Elapsed: 1:14:24.\n",
      "  Batch 6,120  of  7,344.    Elapsed: 1:14:53.\n",
      "  Batch 6,160  of  7,344.    Elapsed: 1:15:23.\n",
      "  Batch 6,200  of  7,344.    Elapsed: 1:15:52.\n",
      "  Batch 6,240  of  7,344.    Elapsed: 1:16:22.\n",
      "  Batch 6,280  of  7,344.    Elapsed: 1:16:51.\n",
      "  Batch 6,320  of  7,344.    Elapsed: 1:17:20.\n",
      "  Batch 6,360  of  7,344.    Elapsed: 1:17:50.\n",
      "  Batch 6,400  of  7,344.    Elapsed: 1:18:19.\n",
      "  Batch 6,440  of  7,344.    Elapsed: 1:18:49.\n",
      "  Batch 6,480  of  7,344.    Elapsed: 1:19:18.\n",
      "  Batch 6,520  of  7,344.    Elapsed: 1:19:47.\n",
      "  Batch 6,560  of  7,344.    Elapsed: 1:20:17.\n",
      "  Batch 6,600  of  7,344.    Elapsed: 1:20:46.\n",
      "  Batch 6,640  of  7,344.    Elapsed: 1:21:15.\n",
      "  Batch 6,680  of  7,344.    Elapsed: 1:21:45.\n",
      "  Batch 6,720  of  7,344.    Elapsed: 1:22:14.\n",
      "  Batch 6,760  of  7,344.    Elapsed: 1:22:43.\n",
      "  Batch 6,800  of  7,344.    Elapsed: 1:23:13.\n",
      "  Batch 6,840  of  7,344.    Elapsed: 1:23:42.\n",
      "  Batch 6,880  of  7,344.    Elapsed: 1:24:12.\n",
      "  Batch 6,920  of  7,344.    Elapsed: 1:24:41.\n",
      "  Batch 6,960  of  7,344.    Elapsed: 1:25:10.\n",
      "  Batch 7,000  of  7,344.    Elapsed: 1:25:40.\n",
      "  Batch 7,040  of  7,344.    Elapsed: 1:26:09.\n",
      "  Batch 7,080  of  7,344.    Elapsed: 1:26:39.\n",
      "  Batch 7,120  of  7,344.    Elapsed: 1:27:08.\n",
      "  Batch 7,160  of  7,344.    Elapsed: 1:27:37.\n",
      "  Batch 7,200  of  7,344.    Elapsed: 1:28:07.\n",
      "  Batch 7,240  of  7,344.    Elapsed: 1:28:36.\n",
      "  Batch 7,280  of  7,344.    Elapsed: 1:29:05.\n",
      "  Batch 7,320  of  7,344.    Elapsed: 1:29:35.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epcoh took: 1:29:52\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.98\n",
      "  Validation Loss: 0.09\n",
      "  Validation took: 0:02:52\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  7,344.    Elapsed: 0:00:29.\n",
      "  Batch    80  of  7,344.    Elapsed: 0:00:59.\n",
      "  Batch   120  of  7,344.    Elapsed: 0:01:28.\n",
      "  Batch   160  of  7,344.    Elapsed: 0:01:57.\n",
      "  Batch   200  of  7,344.    Elapsed: 0:02:27.\n",
      "  Batch   240  of  7,344.    Elapsed: 0:02:56.\n",
      "  Batch   280  of  7,344.    Elapsed: 0:03:25.\n",
      "  Batch   320  of  7,344.    Elapsed: 0:03:55.\n",
      "  Batch   360  of  7,344.    Elapsed: 0:04:24.\n",
      "  Batch   400  of  7,344.    Elapsed: 0:04:53.\n",
      "  Batch   440  of  7,344.    Elapsed: 0:05:23.\n",
      "  Batch   480  of  7,344.    Elapsed: 0:05:52.\n",
      "  Batch   520  of  7,344.    Elapsed: 0:06:21.\n",
      "  Batch   560  of  7,344.    Elapsed: 0:06:51.\n",
      "  Batch   600  of  7,344.    Elapsed: 0:07:20.\n",
      "  Batch   640  of  7,344.    Elapsed: 0:07:49.\n",
      "  Batch   680  of  7,344.    Elapsed: 0:08:19.\n",
      "  Batch   720  of  7,344.    Elapsed: 0:08:48.\n",
      "  Batch   760  of  7,344.    Elapsed: 0:09:17.\n",
      "  Batch   800  of  7,344.    Elapsed: 0:09:47.\n",
      "  Batch   840  of  7,344.    Elapsed: 0:10:16.\n",
      "  Batch   880  of  7,344.    Elapsed: 0:10:45.\n",
      "  Batch   920  of  7,344.    Elapsed: 0:11:15.\n",
      "  Batch   960  of  7,344.    Elapsed: 0:11:44.\n",
      "  Batch 1,000  of  7,344.    Elapsed: 0:12:14.\n",
      "  Batch 1,040  of  7,344.    Elapsed: 0:12:43.\n",
      "  Batch 1,080  of  7,344.    Elapsed: 0:13:12.\n",
      "  Batch 1,120  of  7,344.    Elapsed: 0:13:42.\n",
      "  Batch 1,160  of  7,344.    Elapsed: 0:14:11.\n",
      "  Batch 1,200  of  7,344.    Elapsed: 0:14:40.\n",
      "  Batch 1,240  of  7,344.    Elapsed: 0:15:10.\n",
      "  Batch 1,280  of  7,344.    Elapsed: 0:15:39.\n",
      "  Batch 1,320  of  7,344.    Elapsed: 0:16:09.\n",
      "  Batch 1,360  of  7,344.    Elapsed: 0:16:38.\n",
      "  Batch 1,400  of  7,344.    Elapsed: 0:17:07.\n",
      "  Batch 1,440  of  7,344.    Elapsed: 0:17:37.\n",
      "  Batch 1,480  of  7,344.    Elapsed: 0:18:06.\n",
      "  Batch 1,520  of  7,344.    Elapsed: 0:18:35.\n",
      "  Batch 1,560  of  7,344.    Elapsed: 0:19:05.\n",
      "  Batch 1,600  of  7,344.    Elapsed: 0:19:34.\n",
      "  Batch 1,640  of  7,344.    Elapsed: 0:20:03.\n",
      "  Batch 1,680  of  7,344.    Elapsed: 0:20:33.\n",
      "  Batch 1,720  of  7,344.    Elapsed: 0:21:02.\n",
      "  Batch 1,760  of  7,344.    Elapsed: 0:21:31.\n",
      "  Batch 1,800  of  7,344.    Elapsed: 0:22:01.\n",
      "  Batch 1,840  of  7,344.    Elapsed: 0:22:30.\n",
      "  Batch 1,880  of  7,344.    Elapsed: 0:23:00.\n",
      "  Batch 1,920  of  7,344.    Elapsed: 0:23:29.\n",
      "  Batch 1,960  of  7,344.    Elapsed: 0:23:58.\n",
      "  Batch 2,000  of  7,344.    Elapsed: 0:24:28.\n",
      "  Batch 2,040  of  7,344.    Elapsed: 0:24:57.\n",
      "  Batch 2,080  of  7,344.    Elapsed: 0:25:26.\n",
      "  Batch 2,120  of  7,344.    Elapsed: 0:25:56.\n",
      "  Batch 2,160  of  7,344.    Elapsed: 0:26:25.\n",
      "  Batch 2,200  of  7,344.    Elapsed: 0:26:54.\n",
      "  Batch 2,240  of  7,344.    Elapsed: 0:27:24.\n",
      "  Batch 2,280  of  7,344.    Elapsed: 0:27:53.\n",
      "  Batch 2,320  of  7,344.    Elapsed: 0:28:22.\n",
      "  Batch 2,360  of  7,344.    Elapsed: 0:28:52.\n",
      "  Batch 2,400  of  7,344.    Elapsed: 0:29:21.\n",
      "  Batch 2,440  of  7,344.    Elapsed: 0:29:50.\n",
      "  Batch 2,480  of  7,344.    Elapsed: 0:30:20.\n",
      "  Batch 2,520  of  7,344.    Elapsed: 0:30:49.\n",
      "  Batch 2,560  of  7,344.    Elapsed: 0:31:18.\n",
      "  Batch 2,600  of  7,344.    Elapsed: 0:31:48.\n",
      "  Batch 2,640  of  7,344.    Elapsed: 0:32:17.\n",
      "  Batch 2,680  of  7,344.    Elapsed: 0:32:46.\n",
      "  Batch 2,720  of  7,344.    Elapsed: 0:33:16.\n",
      "  Batch 2,760  of  7,344.    Elapsed: 0:33:45.\n",
      "  Batch 2,800  of  7,344.    Elapsed: 0:34:15.\n",
      "  Batch 2,840  of  7,344.    Elapsed: 0:34:44.\n",
      "  Batch 2,880  of  7,344.    Elapsed: 0:35:13.\n",
      "  Batch 2,920  of  7,344.    Elapsed: 0:35:43.\n",
      "  Batch 2,960  of  7,344.    Elapsed: 0:36:12.\n",
      "  Batch 3,000  of  7,344.    Elapsed: 0:36:41.\n",
      "  Batch 3,040  of  7,344.    Elapsed: 0:37:11.\n",
      "  Batch 3,080  of  7,344.    Elapsed: 0:37:40.\n",
      "  Batch 3,120  of  7,344.    Elapsed: 0:38:09.\n",
      "  Batch 3,160  of  7,344.    Elapsed: 0:38:39.\n",
      "  Batch 3,200  of  7,344.    Elapsed: 0:39:08.\n",
      "  Batch 3,240  of  7,344.    Elapsed: 0:39:37.\n",
      "  Batch 3,280  of  7,344.    Elapsed: 0:40:07.\n",
      "  Batch 3,320  of  7,344.    Elapsed: 0:40:36.\n",
      "  Batch 3,360  of  7,344.    Elapsed: 0:41:05.\n",
      "  Batch 3,400  of  7,344.    Elapsed: 0:41:35.\n",
      "  Batch 3,440  of  7,344.    Elapsed: 0:42:04.\n",
      "  Batch 3,480  of  7,344.    Elapsed: 0:42:33.\n",
      "  Batch 3,520  of  7,344.    Elapsed: 0:43:03.\n",
      "  Batch 3,560  of  7,344.    Elapsed: 0:43:32.\n",
      "  Batch 3,600  of  7,344.    Elapsed: 0:44:01.\n",
      "  Batch 3,640  of  7,344.    Elapsed: 0:44:31.\n",
      "  Batch 3,680  of  7,344.    Elapsed: 0:45:00.\n",
      "  Batch 3,720  of  7,344.    Elapsed: 0:45:29.\n",
      "  Batch 3,760  of  7,344.    Elapsed: 0:45:59.\n",
      "  Batch 3,800  of  7,344.    Elapsed: 0:46:28.\n",
      "  Batch 3,840  of  7,344.    Elapsed: 0:46:57.\n",
      "  Batch 3,880  of  7,344.    Elapsed: 0:47:27.\n",
      "  Batch 3,920  of  7,344.    Elapsed: 0:47:56.\n",
      "  Batch 3,960  of  7,344.    Elapsed: 0:48:25.\n",
      "  Batch 4,000  of  7,344.    Elapsed: 0:48:55.\n",
      "  Batch 4,040  of  7,344.    Elapsed: 0:49:24.\n",
      "  Batch 4,080  of  7,344.    Elapsed: 0:49:53.\n",
      "  Batch 4,120  of  7,344.    Elapsed: 0:50:23.\n",
      "  Batch 4,160  of  7,344.    Elapsed: 0:50:52.\n",
      "  Batch 4,200  of  7,344.    Elapsed: 0:51:21.\n",
      "  Batch 4,240  of  7,344.    Elapsed: 0:51:51.\n",
      "  Batch 4,280  of  7,344.    Elapsed: 0:52:20.\n",
      "  Batch 4,320  of  7,344.    Elapsed: 0:52:49.\n",
      "  Batch 4,360  of  7,344.    Elapsed: 0:53:19.\n",
      "  Batch 4,400  of  7,344.    Elapsed: 0:53:48.\n",
      "  Batch 4,440  of  7,344.    Elapsed: 0:54:17.\n",
      "  Batch 4,480  of  7,344.    Elapsed: 0:54:47.\n",
      "  Batch 4,520  of  7,344.    Elapsed: 0:55:16.\n",
      "  Batch 4,560  of  7,344.    Elapsed: 0:55:45.\n",
      "  Batch 4,600  of  7,344.    Elapsed: 0:56:14.\n",
      "  Batch 4,640  of  7,344.    Elapsed: 0:56:44.\n",
      "  Batch 4,680  of  7,344.    Elapsed: 0:57:13.\n",
      "  Batch 4,720  of  7,344.    Elapsed: 0:57:42.\n",
      "  Batch 4,760  of  7,344.    Elapsed: 0:58:12.\n",
      "  Batch 4,800  of  7,344.    Elapsed: 0:58:41.\n",
      "  Batch 4,840  of  7,344.    Elapsed: 0:59:10.\n",
      "  Batch 4,880  of  7,344.    Elapsed: 0:59:40.\n",
      "  Batch 4,920  of  7,344.    Elapsed: 1:00:09.\n",
      "  Batch 4,960  of  7,344.    Elapsed: 1:00:38.\n",
      "  Batch 5,000  of  7,344.    Elapsed: 1:01:08.\n",
      "  Batch 5,040  of  7,344.    Elapsed: 1:01:37.\n",
      "  Batch 5,080  of  7,344.    Elapsed: 1:02:06.\n",
      "  Batch 5,120  of  7,344.    Elapsed: 1:02:36.\n",
      "  Batch 5,160  of  7,344.    Elapsed: 1:03:05.\n",
      "  Batch 5,200  of  7,344.    Elapsed: 1:03:34.\n",
      "  Batch 5,240  of  7,344.    Elapsed: 1:04:04.\n",
      "  Batch 5,280  of  7,344.    Elapsed: 1:04:33.\n",
      "  Batch 5,320  of  7,344.    Elapsed: 1:05:02.\n",
      "  Batch 5,360  of  7,344.    Elapsed: 1:05:32.\n",
      "  Batch 5,400  of  7,344.    Elapsed: 1:06:01.\n",
      "  Batch 5,440  of  7,344.    Elapsed: 1:06:30.\n",
      "  Batch 5,480  of  7,344.    Elapsed: 1:07:00.\n",
      "  Batch 5,520  of  7,344.    Elapsed: 1:07:29.\n",
      "  Batch 5,560  of  7,344.    Elapsed: 1:07:58.\n",
      "  Batch 5,600  of  7,344.    Elapsed: 1:08:28.\n",
      "  Batch 5,640  of  7,344.    Elapsed: 1:08:57.\n",
      "  Batch 5,680  of  7,344.    Elapsed: 1:09:26.\n",
      "  Batch 5,720  of  7,344.    Elapsed: 1:09:56.\n",
      "  Batch 5,760  of  7,344.    Elapsed: 1:10:25.\n",
      "  Batch 5,800  of  7,344.    Elapsed: 1:10:54.\n",
      "  Batch 5,840  of  7,344.    Elapsed: 1:11:24.\n",
      "  Batch 5,880  of  7,344.    Elapsed: 1:11:53.\n",
      "  Batch 5,920  of  7,344.    Elapsed: 1:12:22.\n",
      "  Batch 5,960  of  7,344.    Elapsed: 1:12:52.\n",
      "  Batch 6,000  of  7,344.    Elapsed: 1:13:21.\n",
      "  Batch 6,040  of  7,344.    Elapsed: 1:13:50.\n",
      "  Batch 6,080  of  7,344.    Elapsed: 1:14:20.\n",
      "  Batch 6,120  of  7,344.    Elapsed: 1:14:49.\n",
      "  Batch 6,160  of  7,344.    Elapsed: 1:15:18.\n",
      "  Batch 6,200  of  7,344.    Elapsed: 1:15:48.\n",
      "  Batch 6,240  of  7,344.    Elapsed: 1:16:17.\n",
      "  Batch 6,280  of  7,344.    Elapsed: 1:16:46.\n",
      "  Batch 6,320  of  7,344.    Elapsed: 1:17:16.\n",
      "  Batch 6,360  of  7,344.    Elapsed: 1:17:45.\n",
      "  Batch 6,400  of  7,344.    Elapsed: 1:18:14.\n",
      "  Batch 6,440  of  7,344.    Elapsed: 1:18:44.\n",
      "  Batch 6,480  of  7,344.    Elapsed: 1:19:13.\n",
      "  Batch 6,520  of  7,344.    Elapsed: 1:19:42.\n",
      "  Batch 6,560  of  7,344.    Elapsed: 1:20:11.\n",
      "  Batch 6,600  of  7,344.    Elapsed: 1:20:41.\n",
      "  Batch 6,640  of  7,344.    Elapsed: 1:21:10.\n",
      "  Batch 6,680  of  7,344.    Elapsed: 1:21:39.\n",
      "  Batch 6,720  of  7,344.    Elapsed: 1:22:09.\n",
      "  Batch 6,760  of  7,344.    Elapsed: 1:22:38.\n",
      "  Batch 6,800  of  7,344.    Elapsed: 1:23:07.\n",
      "  Batch 6,840  of  7,344.    Elapsed: 1:23:37.\n",
      "  Batch 6,880  of  7,344.    Elapsed: 1:24:06.\n",
      "  Batch 6,920  of  7,344.    Elapsed: 1:24:35.\n",
      "  Batch 6,960  of  7,344.    Elapsed: 1:25:05.\n",
      "  Batch 7,000  of  7,344.    Elapsed: 1:25:34.\n",
      "  Batch 7,040  of  7,344.    Elapsed: 1:26:03.\n",
      "  Batch 7,080  of  7,344.    Elapsed: 1:26:32.\n",
      "  Batch 7,120  of  7,344.    Elapsed: 1:27:02.\n",
      "  Batch 7,160  of  7,344.    Elapsed: 1:27:31.\n",
      "  Batch 7,200  of  7,344.    Elapsed: 1:28:00.\n",
      "  Batch 7,240  of  7,344.    Elapsed: 1:28:30.\n",
      "  Batch 7,280  of  7,344.    Elapsed: 1:28:59.\n",
      "  Batch 7,320  of  7,344.    Elapsed: 1:29:28.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epcoh took: 1:29:46\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:02:52\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  7,344.    Elapsed: 0:00:29.\n",
      "  Batch    80  of  7,344.    Elapsed: 0:00:59.\n",
      "  Batch   120  of  7,344.    Elapsed: 0:01:28.\n",
      "  Batch   160  of  7,344.    Elapsed: 0:01:57.\n",
      "  Batch   200  of  7,344.    Elapsed: 0:02:26.\n",
      "  Batch   240  of  7,344.    Elapsed: 0:02:56.\n",
      "  Batch   280  of  7,344.    Elapsed: 0:03:25.\n",
      "  Batch   320  of  7,344.    Elapsed: 0:03:54.\n",
      "  Batch   360  of  7,344.    Elapsed: 0:04:24.\n",
      "  Batch   400  of  7,344.    Elapsed: 0:04:53.\n",
      "  Batch   440  of  7,344.    Elapsed: 0:05:22.\n",
      "  Batch   480  of  7,344.    Elapsed: 0:05:52.\n",
      "  Batch   520  of  7,344.    Elapsed: 0:06:21.\n",
      "  Batch   560  of  7,344.    Elapsed: 0:06:50.\n",
      "  Batch   600  of  7,344.    Elapsed: 0:07:20.\n",
      "  Batch   640  of  7,344.    Elapsed: 0:07:49.\n",
      "  Batch   680  of  7,344.    Elapsed: 0:08:18.\n",
      "  Batch   720  of  7,344.    Elapsed: 0:08:47.\n",
      "  Batch   760  of  7,344.    Elapsed: 0:09:17.\n",
      "  Batch   800  of  7,344.    Elapsed: 0:09:46.\n",
      "  Batch   840  of  7,344.    Elapsed: 0:10:15.\n",
      "  Batch   880  of  7,344.    Elapsed: 0:10:45.\n",
      "  Batch   920  of  7,344.    Elapsed: 0:11:14.\n",
      "  Batch   960  of  7,344.    Elapsed: 0:11:43.\n",
      "  Batch 1,000  of  7,344.    Elapsed: 0:12:12.\n",
      "  Batch 1,040  of  7,344.    Elapsed: 0:12:42.\n",
      "  Batch 1,080  of  7,344.    Elapsed: 0:13:11.\n",
      "  Batch 1,120  of  7,344.    Elapsed: 0:13:40.\n",
      "  Batch 1,160  of  7,344.    Elapsed: 0:14:10.\n",
      "  Batch 1,200  of  7,344.    Elapsed: 0:14:39.\n",
      "  Batch 1,240  of  7,344.    Elapsed: 0:15:08.\n",
      "  Batch 1,280  of  7,344.    Elapsed: 0:15:37.\n",
      "  Batch 1,320  of  7,344.    Elapsed: 0:16:07.\n",
      "  Batch 1,360  of  7,344.    Elapsed: 0:16:36.\n",
      "  Batch 1,400  of  7,344.    Elapsed: 0:17:05.\n",
      "  Batch 1,440  of  7,344.    Elapsed: 0:17:34.\n",
      "  Batch 1,480  of  7,344.    Elapsed: 0:18:04.\n",
      "  Batch 1,520  of  7,344.    Elapsed: 0:18:33.\n",
      "  Batch 1,560  of  7,344.    Elapsed: 0:19:02.\n",
      "  Batch 1,600  of  7,344.    Elapsed: 0:19:32.\n",
      "  Batch 1,640  of  7,344.    Elapsed: 0:20:01.\n",
      "  Batch 1,680  of  7,344.    Elapsed: 0:20:30.\n",
      "  Batch 1,720  of  7,344.    Elapsed: 0:21:00.\n",
      "  Batch 1,760  of  7,344.    Elapsed: 0:21:29.\n",
      "  Batch 1,800  of  7,344.    Elapsed: 0:21:58.\n",
      "  Batch 1,840  of  7,344.    Elapsed: 0:22:27.\n",
      "  Batch 1,880  of  7,344.    Elapsed: 0:22:57.\n",
      "  Batch 1,920  of  7,344.    Elapsed: 0:23:26.\n",
      "  Batch 1,960  of  7,344.    Elapsed: 0:23:55.\n",
      "  Batch 2,000  of  7,344.    Elapsed: 0:24:25.\n",
      "  Batch 2,040  of  7,344.    Elapsed: 0:24:54.\n",
      "  Batch 2,080  of  7,344.    Elapsed: 0:25:23.\n",
      "  Batch 2,120  of  7,344.    Elapsed: 0:25:52.\n",
      "  Batch 2,160  of  7,344.    Elapsed: 0:26:22.\n",
      "  Batch 2,200  of  7,344.    Elapsed: 0:26:51.\n",
      "  Batch 2,240  of  7,344.    Elapsed: 0:27:20.\n",
      "  Batch 2,280  of  7,344.    Elapsed: 0:27:50.\n",
      "  Batch 2,320  of  7,344.    Elapsed: 0:28:19.\n",
      "  Batch 2,360  of  7,344.    Elapsed: 0:28:48.\n",
      "  Batch 2,400  of  7,344.    Elapsed: 0:29:17.\n",
      "  Batch 2,440  of  7,344.    Elapsed: 0:29:47.\n",
      "  Batch 2,480  of  7,344.    Elapsed: 0:30:16.\n",
      "  Batch 2,520  of  7,344.    Elapsed: 0:30:45.\n",
      "  Batch 2,560  of  7,344.    Elapsed: 0:31:15.\n",
      "  Batch 2,600  of  7,344.    Elapsed: 0:31:44.\n",
      "  Batch 2,640  of  7,344.    Elapsed: 0:32:13.\n",
      "  Batch 2,680  of  7,344.    Elapsed: 0:32:42.\n",
      "  Batch 2,720  of  7,344.    Elapsed: 0:33:12.\n",
      "  Batch 2,760  of  7,344.    Elapsed: 0:33:41.\n",
      "  Batch 2,800  of  7,344.    Elapsed: 0:34:10.\n",
      "  Batch 2,840  of  7,344.    Elapsed: 0:34:40.\n",
      "  Batch 2,880  of  7,344.    Elapsed: 0:35:09.\n",
      "  Batch 2,920  of  7,344.    Elapsed: 0:35:38.\n",
      "  Batch 2,960  of  7,344.    Elapsed: 0:36:07.\n",
      "  Batch 3,000  of  7,344.    Elapsed: 0:36:37.\n",
      "  Batch 3,040  of  7,344.    Elapsed: 0:37:06.\n",
      "  Batch 3,080  of  7,344.    Elapsed: 0:37:35.\n",
      "  Batch 3,120  of  7,344.    Elapsed: 0:38:05.\n",
      "  Batch 3,160  of  7,344.    Elapsed: 0:38:34.\n",
      "  Batch 3,200  of  7,344.    Elapsed: 0:39:03.\n",
      "  Batch 3,240  of  7,344.    Elapsed: 0:39:33.\n",
      "  Batch 3,280  of  7,344.    Elapsed: 0:40:02.\n",
      "  Batch 3,320  of  7,344.    Elapsed: 0:40:31.\n",
      "  Batch 3,360  of  7,344.    Elapsed: 0:41:00.\n",
      "  Batch 3,400  of  7,344.    Elapsed: 0:41:30.\n",
      "  Batch 3,440  of  7,344.    Elapsed: 0:41:59.\n",
      "  Batch 3,480  of  7,344.    Elapsed: 0:42:28.\n",
      "  Batch 3,520  of  7,344.    Elapsed: 0:42:58.\n",
      "  Batch 3,560  of  7,344.    Elapsed: 0:43:27.\n",
      "  Batch 3,600  of  7,344.    Elapsed: 0:43:56.\n",
      "  Batch 3,640  of  7,344.    Elapsed: 0:44:25.\n",
      "  Batch 3,680  of  7,344.    Elapsed: 0:44:55.\n",
      "  Batch 3,720  of  7,344.    Elapsed: 0:45:24.\n",
      "  Batch 3,760  of  7,344.    Elapsed: 0:45:53.\n",
      "  Batch 3,800  of  7,344.    Elapsed: 0:46:23.\n",
      "  Batch 3,840  of  7,344.    Elapsed: 0:46:52.\n",
      "  Batch 3,880  of  7,344.    Elapsed: 0:47:21.\n",
      "  Batch 3,920  of  7,344.    Elapsed: 0:47:50.\n",
      "  Batch 3,960  of  7,344.    Elapsed: 0:48:20.\n",
      "  Batch 4,000  of  7,344.    Elapsed: 0:48:49.\n",
      "  Batch 4,040  of  7,344.    Elapsed: 0:49:18.\n",
      "  Batch 4,080  of  7,344.    Elapsed: 0:49:48.\n",
      "  Batch 4,120  of  7,344.    Elapsed: 0:50:17.\n",
      "  Batch 4,160  of  7,344.    Elapsed: 0:50:46.\n",
      "  Batch 4,200  of  7,344.    Elapsed: 0:51:15.\n",
      "  Batch 4,240  of  7,344.    Elapsed: 0:51:45.\n",
      "  Batch 4,280  of  7,344.    Elapsed: 0:52:14.\n",
      "  Batch 4,320  of  7,344.    Elapsed: 0:52:43.\n",
      "  Batch 4,360  of  7,344.    Elapsed: 0:53:13.\n",
      "  Batch 4,400  of  7,344.    Elapsed: 0:53:42.\n",
      "  Batch 4,440  of  7,344.    Elapsed: 0:54:11.\n",
      "  Batch 4,480  of  7,344.    Elapsed: 0:54:40.\n",
      "  Batch 4,520  of  7,344.    Elapsed: 0:55:10.\n",
      "  Batch 4,560  of  7,344.    Elapsed: 0:55:39.\n",
      "  Batch 4,600  of  7,344.    Elapsed: 0:56:08.\n",
      "  Batch 4,640  of  7,344.    Elapsed: 0:56:37.\n",
      "  Batch 4,680  of  7,344.    Elapsed: 0:57:07.\n",
      "  Batch 4,720  of  7,344.    Elapsed: 0:57:36.\n",
      "  Batch 4,760  of  7,344.    Elapsed: 0:58:05.\n",
      "  Batch 4,800  of  7,344.    Elapsed: 0:58:35.\n",
      "  Batch 4,840  of  7,344.    Elapsed: 0:59:04.\n",
      "  Batch 4,880  of  7,344.    Elapsed: 0:59:33.\n",
      "  Batch 4,920  of  7,344.    Elapsed: 1:00:02.\n",
      "  Batch 4,960  of  7,344.    Elapsed: 1:00:32.\n",
      "  Batch 5,000  of  7,344.    Elapsed: 1:01:01.\n",
      "  Batch 5,040  of  7,344.    Elapsed: 1:01:30.\n",
      "  Batch 5,080  of  7,344.    Elapsed: 1:01:59.\n",
      "  Batch 5,120  of  7,344.    Elapsed: 1:02:29.\n",
      "  Batch 5,160  of  7,344.    Elapsed: 1:02:58.\n",
      "  Batch 5,200  of  7,344.    Elapsed: 1:03:27.\n",
      "  Batch 5,240  of  7,344.    Elapsed: 1:03:56.\n",
      "  Batch 5,280  of  7,344.    Elapsed: 1:04:26.\n",
      "  Batch 5,320  of  7,344.    Elapsed: 1:04:55.\n",
      "  Batch 5,360  of  7,344.    Elapsed: 1:05:24.\n",
      "  Batch 5,400  of  7,344.    Elapsed: 1:05:54.\n",
      "  Batch 5,440  of  7,344.    Elapsed: 1:06:23.\n",
      "  Batch 5,480  of  7,344.    Elapsed: 1:06:52.\n",
      "  Batch 5,520  of  7,344.    Elapsed: 1:07:21.\n",
      "  Batch 5,560  of  7,344.    Elapsed: 1:07:51.\n",
      "  Batch 5,600  of  7,344.    Elapsed: 1:08:20.\n",
      "  Batch 5,640  of  7,344.    Elapsed: 1:08:49.\n",
      "  Batch 5,680  of  7,344.    Elapsed: 1:09:19.\n",
      "  Batch 5,720  of  7,344.    Elapsed: 1:09:48.\n",
      "  Batch 5,760  of  7,344.    Elapsed: 1:10:17.\n",
      "  Batch 5,800  of  7,344.    Elapsed: 1:10:46.\n",
      "  Batch 5,840  of  7,344.    Elapsed: 1:11:16.\n",
      "  Batch 5,880  of  7,344.    Elapsed: 1:11:45.\n",
      "  Batch 5,920  of  7,344.    Elapsed: 1:12:14.\n",
      "  Batch 5,960  of  7,344.    Elapsed: 1:12:43.\n",
      "  Batch 6,000  of  7,344.    Elapsed: 1:13:13.\n",
      "  Batch 6,040  of  7,344.    Elapsed: 1:13:42.\n",
      "  Batch 6,080  of  7,344.    Elapsed: 1:14:11.\n",
      "  Batch 6,120  of  7,344.    Elapsed: 1:14:41.\n",
      "  Batch 6,160  of  7,344.    Elapsed: 1:15:10.\n",
      "  Batch 6,200  of  7,344.    Elapsed: 1:15:39.\n",
      "  Batch 6,240  of  7,344.    Elapsed: 1:16:08.\n",
      "  Batch 6,280  of  7,344.    Elapsed: 1:16:38.\n",
      "  Batch 6,320  of  7,344.    Elapsed: 1:17:07.\n",
      "  Batch 6,360  of  7,344.    Elapsed: 1:17:36.\n",
      "  Batch 6,400  of  7,344.    Elapsed: 1:18:06.\n",
      "  Batch 6,440  of  7,344.    Elapsed: 1:18:35.\n",
      "  Batch 6,480  of  7,344.    Elapsed: 1:19:04.\n",
      "  Batch 6,520  of  7,344.    Elapsed: 1:19:34.\n",
      "  Batch 6,560  of  7,344.    Elapsed: 1:20:03.\n",
      "  Batch 6,600  of  7,344.    Elapsed: 1:20:32.\n",
      "  Batch 6,640  of  7,344.    Elapsed: 1:21:01.\n",
      "  Batch 6,680  of  7,344.    Elapsed: 1:21:31.\n",
      "  Batch 6,720  of  7,344.    Elapsed: 1:22:00.\n",
      "  Batch 6,760  of  7,344.    Elapsed: 1:22:29.\n",
      "  Batch 6,800  of  7,344.    Elapsed: 1:22:59.\n",
      "  Batch 6,840  of  7,344.    Elapsed: 1:23:28.\n",
      "  Batch 6,880  of  7,344.    Elapsed: 1:23:57.\n",
      "  Batch 6,920  of  7,344.    Elapsed: 1:24:26.\n",
      "  Batch 6,960  of  7,344.    Elapsed: 1:24:56.\n",
      "  Batch 7,000  of  7,344.    Elapsed: 1:25:25.\n",
      "  Batch 7,040  of  7,344.    Elapsed: 1:25:54.\n",
      "  Batch 7,080  of  7,344.    Elapsed: 1:26:24.\n",
      "  Batch 7,120  of  7,344.    Elapsed: 1:26:53.\n",
      "  Batch 7,160  of  7,344.    Elapsed: 1:27:22.\n",
      "  Batch 7,200  of  7,344.    Elapsed: 1:27:52.\n",
      "  Batch 7,240  of  7,344.    Elapsed: 1:28:21.\n",
      "  Batch 7,280  of  7,344.    Elapsed: 1:28:50.\n",
      "  Batch 7,320  of  7,344.    Elapsed: 1:29:19.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 1:29:37\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99\n",
      "  Validation Loss: 0.03\n",
      "  Validation took: 0:02:52\n",
      "\n",
      "Training complete!\n",
      "Total training took 6:10:37 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "### TRAIN MODEL\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss_logit = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        loss = loss_logit[0]\n",
    "        logit = loss_logit[1]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            loss_logit = model(b_input_ids, \n",
    "                                 token_type_ids=None, \n",
    "                                 attention_mask=b_input_mask, \n",
    "                                 labels=b_labels)\n",
    "            loss = loss_logit[0]\n",
    "            logits = loss_logit[1]\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9ddfedd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T23:42:21.166911Z",
     "iopub.status.busy": "2022-04-09T23:42:21.166154Z",
     "iopub.status.idle": "2022-04-09T23:42:22.531203Z",
     "shell.execute_reply": "2022-04-09T23:42:22.531597Z",
     "shell.execute_reply.started": "2022-04-09T14:16:48.273804Z"
    },
    "papermill": {
     "duration": 1.606959,
     "end_time": "2022-04-09T23:42:22.531745",
     "exception": false,
     "start_time": "2022-04-09T23:42:20.924786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/tokenizer_config.json',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/vocab.txt',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SAVE FINE TUNED MODEL\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb96284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T23:42:23.024882Z",
     "iopub.status.busy": "2022-04-09T23:42:23.024304Z",
     "iopub.status.idle": "2022-04-09T23:42:23.753904Z",
     "shell.execute_reply": "2022-04-09T23:42:23.753377Z",
     "shell.execute_reply.started": "2022-04-09T14:17:09.149652Z"
    },
    "papermill": {
     "duration": 0.985081,
     "end_time": "2022-04-09T23:42:23.754041",
     "exception": false,
     "start_time": "2022-04-09T23:42:22.768960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1:29:55</td>\n",
       "      <td>0:02:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1:29:52</td>\n",
       "      <td>0:02:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1:29:46</td>\n",
       "      <td>0:02:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1:29:37</td>\n",
       "      <td>0:02:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.90         0.16           0.96       1:29:55         0:02:52\n",
       "2               0.15         0.09           0.98       1:29:52         0:02:52\n",
       "3               0.08         0.04           0.99       1:29:46         0:02:52\n",
       "4               0.04         0.03           0.99       1:29:37         0:02:52"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PLOT LOSS\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a65ac65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T23:42:24.562627Z",
     "iopub.status.busy": "2022-04-09T23:42:24.561552Z",
     "iopub.status.idle": "2022-04-10T00:11:02.165521Z",
     "shell.execute_reply": "2022-04-10T00:11:02.166005Z",
     "shell.execute_reply.started": "2022-04-09T14:17:23.879002Z"
    },
    "papermill": {
     "duration": 1718.017958,
     "end_time": "2022-04-10T00:11:02.166168",
     "exception": false,
     "start_time": "2022-04-09T23:42:24.148210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,mwl\n",
      "1,nld\n",
      "2,ava\n",
      "3,tcy\n",
      "4,bjn\n",
      "5,mon\n",
      "6,glk\n",
      "7,lez\n",
      "8,bul\n",
      "9,nan\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "### MAKE PREDICTIONS\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions = []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, _ = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "\n",
    "preds = np.concatenate(predictions).argmax(1).tolist()\n",
    "\n",
    "with open('submission.csv', 'w') as f:\n",
    "    print('Id,Language', file=f)\n",
    "    for sentence_id, lang_id in enumerate(preds):\n",
    "        language = lang_vocab.idx2token[lang_id]\n",
    "        if sentence_id < 10:\n",
    "            print(f'{sentence_id},{language}')\n",
    "        print(f'{sentence_id},{language}', file=f)\n",
    "    \n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f5f4cf",
   "metadata": {
    "papermill": {
     "duration": 0.371773,
     "end_time": "2022-04-10T00:11:02.911600",
     "exception": false,
     "start_time": "2022-04-10T00:11:02.539827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24687.988788,
   "end_time": "2022-04-10T00:11:06.362515",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-09T17:19:38.373727",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "007eb3dfcfb143909831b7dbe8bf0cdf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "02d7bbb340c440d7bfecec708419349e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_506f436d59b84985878a6abbefb7b2ba",
       "placeholder": "​",
       "style": "IPY_MODEL_e3dc50c6fb2b4ef18591f949426635ac",
       "value": "Downloading: 100%"
      }
     },
     "04d2baac2f2e4be9b60b592c5c0b4473": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "083e6b2ac71d4e2a93576ce978cb089e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0ac44f63879a42168dfb238483122947": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_78e4f6398c5040b283fa6d6e7465c6d2",
        "IPY_MODEL_1b3aa57e32c946cb94de3eea2fe3d74c",
        "IPY_MODEL_7fe95777744c4f35988fe2053dbc5beb"
       ],
       "layout": "IPY_MODEL_46df90b59b404e05acf95281095f1baa"
      }
     },
     "0de25af2e3e24556854df4d3c6891d64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "11bda16309274cea974c42e6622ed6bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "12c21090982b46ffb22d2eb8b909a519": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ee10956bb9874041875e45f91987f63d",
       "max": 625.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_131bda304eca4485b6e30ed425fb1f17",
       "value": 625.0
      }
     },
     "131bda304eca4485b6e30ed425fb1f17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "158f63b9a76741bba9af3b297a6001d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_459d09256d0642dea01a09557d969621",
        "IPY_MODEL_12c21090982b46ffb22d2eb8b909a519",
        "IPY_MODEL_c3ec288b52b24a3d9a1ee1a43b6a6f8c"
       ],
       "layout": "IPY_MODEL_4fb5207603914fed9e90b244fc90c099"
      }
     },
     "1b3aa57e32c946cb94de3eea2fe3d74c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4b3f2e0b509b4f60bbfed7f458ae7467",
       "max": 1715180.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_11bda16309274cea974c42e6622ed6bc",
       "value": 1715180.0
      }
     },
     "1d0de7ada7754f8d8b196b0fbc9e673a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b5c67d940f50441d9144f4a028fba135",
       "placeholder": "​",
       "style": "IPY_MODEL_52bf6d0e017c492080320d84b17468ab",
       "value": " 28.0/28.0 [00:00&lt;00:00, 1.10kB/s]"
      }
     },
     "273df839b0274fc69e637d7a0c346b75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7d6566a488674fe2bb01206f11f5a8e1",
       "placeholder": "​",
       "style": "IPY_MODEL_62f62f0cf48d4dcab5761fb81336f979",
       "value": " 851k/851k [00:00&lt;00:00, 1.87MB/s]"
      }
     },
     "41ce9e96842448dfb14b184eab035b8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_02d7bbb340c440d7bfecec708419349e",
        "IPY_MODEL_a5a3d9fe591f4df2bf19efedd1fc75b8",
        "IPY_MODEL_90a756ebabef4ebf8dc757ac738cdb45"
       ],
       "layout": "IPY_MODEL_b798dd3f01864e9788a0f1cec6f30235"
      }
     },
     "459d09256d0642dea01a09557d969621": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_72ce72c23f4841668881d2cc47d7add7",
       "placeholder": "​",
       "style": "IPY_MODEL_6d1faf50fd594179b90a4b95ed70905b",
       "value": "Downloading: 100%"
      }
     },
     "46df90b59b404e05acf95281095f1baa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4b3f2e0b509b4f60bbfed7f458ae7467": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4fb5207603914fed9e90b244fc90c099": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "506f436d59b84985878a6abbefb7b2ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "52bf6d0e017c492080320d84b17468ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5f508b747a2d4e68b64f546d99c39788": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_eac40fd5a8ce4b4bbe1d0f44aa42dd95",
        "IPY_MODEL_ed7a12cb088a454b959fc9bac0929bbf",
        "IPY_MODEL_1d0de7ada7754f8d8b196b0fbc9e673a"
       ],
       "layout": "IPY_MODEL_9d72a83cfad54a6a9d0b12f22f214536"
      }
     },
     "62f62f0cf48d4dcab5761fb81336f979": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6d1faf50fd594179b90a4b95ed70905b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "72ce72c23f4841668881d2cc47d7add7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "78e4f6398c5040b283fa6d6e7465c6d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_da95b299f6d044b5ad71be589b6d25ab",
       "placeholder": "​",
       "style": "IPY_MODEL_eea910607217489597ba00992472f3e8",
       "value": "Downloading: 100%"
      }
     },
     "7d6566a488674fe2bb01206f11f5a8e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7fe95777744c4f35988fe2053dbc5beb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b2ab9c4c217d474db167e558cdbe7c05",
       "placeholder": "​",
       "style": "IPY_MODEL_ca6c0580984c47bd8e73f59e6a90b447",
       "value": " 1.64M/1.64M [00:00&lt;00:00, 5.22MB/s]"
      }
     },
     "81c534a84d9c403f893a48d9cde12111": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8a598b7d351145738087d70fbe9a03e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8bb917045012422780084c013a95fbc3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "90a756ebabef4ebf8dc757ac738cdb45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_007eb3dfcfb143909831b7dbe8bf0cdf",
       "placeholder": "​",
       "style": "IPY_MODEL_04d2baac2f2e4be9b60b592c5c0b4473",
       "value": " 641M/641M [00:22&lt;00:00, 31.4MB/s]"
      }
     },
     "9d72a83cfad54a6a9d0b12f22f214536": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a097c7e911aa40039cf93c19b716c11f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_81c534a84d9c403f893a48d9cde12111",
       "placeholder": "​",
       "style": "IPY_MODEL_083e6b2ac71d4e2a93576ce978cb089e",
       "value": "Downloading: 100%"
      }
     },
     "a0d189db0b524585ac0210881201fb4a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a5a3d9fe591f4df2bf19efedd1fc75b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_aa1cda9876444390b5dd960223299d35",
       "max": 672271273.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fadadededc524bb6b959d1f5164e11e4",
       "value": 672271273.0
      }
     },
     "a7174545995a41ba877a3af7bd079233": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa1cda9876444390b5dd960223299d35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b2ab9c4c217d474db167e558cdbe7c05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b5c67d940f50441d9144f4a028fba135": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b798dd3f01864e9788a0f1cec6f30235": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c3ec288b52b24a3d9a1ee1a43b6a6f8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fd171910411b4230bf118a016a356e82",
       "placeholder": "​",
       "style": "IPY_MODEL_f4bf5a78f3a745aa91455742608ffbdd",
       "value": " 625/625 [00:00&lt;00:00, 24.5kB/s]"
      }
     },
     "ca66a95a6aa642d6ac56a77bdab42b25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a0d189db0b524585ac0210881201fb4a",
       "max": 871891.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d03b98e322d1435d892d1810c93c8063",
       "value": 871891.0
      }
     },
     "ca6c0580984c47bd8e73f59e6a90b447": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ce400047a97444d2996c4369cd9b432e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a097c7e911aa40039cf93c19b716c11f",
        "IPY_MODEL_ca66a95a6aa642d6ac56a77bdab42b25",
        "IPY_MODEL_273df839b0274fc69e637d7a0c346b75"
       ],
       "layout": "IPY_MODEL_8bb917045012422780084c013a95fbc3"
      }
     },
     "d03b98e322d1435d892d1810c93c8063": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "da95b299f6d044b5ad71be589b6d25ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e3dc50c6fb2b4ef18591f949426635ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e5d48fb8667e414ca6c38c7c9165733a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "eac40fd5a8ce4b4bbe1d0f44aa42dd95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a7174545995a41ba877a3af7bd079233",
       "placeholder": "​",
       "style": "IPY_MODEL_e5d48fb8667e414ca6c38c7c9165733a",
       "value": "Downloading: 100%"
      }
     },
     "ed7a12cb088a454b959fc9bac0929bbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8a598b7d351145738087d70fbe9a03e3",
       "max": 28.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0de25af2e3e24556854df4d3c6891d64",
       "value": 28.0
      }
     },
     "ee10956bb9874041875e45f91987f63d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eea910607217489597ba00992472f3e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f4bf5a78f3a745aa91455742608ffbdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fadadededc524bb6b959d1f5164e11e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fd171910411b4230bf118a016a356e82": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
